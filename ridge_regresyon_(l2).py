# -*- coding: utf-8 -*-
"""Ridge Regresyon (L2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13qDDGZRxr80eP7LfCvq3tieCodhY3N4o

**Ridge Regresyon overfitting** (Aşırı öğrenme durumları için kullanılır.)
Ridge Regresyon sayesinde bias ve varyans arasındaki dengeyi sağlayabiliriz.

Ridge Regresyon sayesinde bias ve varyans arasındaki dengeyi sağlayabiliriz.

Ridge Regresyonda katsayılar üzerinde regülasyon yapılıyor.

Ridge regresyonda katsayılar küçülür ama sıfır olmaz. Features (öz nitelik)) azalmaz.

Ridge Regresyonda cezalar karesi ile orantılıdır.
"""

# y = a1*x1+a2*x2+....+b+alfa* (katsayılar toplamı)**2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression,Ridge
#from sklearn.linear_model import Ridge

df=pd.read_csv("student_scores.csv")

df.head(3)

y=df["Scores"]
#x=df.drop("Scores",axis=1)
x=df[["Hours"]]

plt.style.use("seaborn")
plt.figure(figsize=(10,6))
plt.scatter(x,y)
plt.show()

lr=LinearRegression()
model=lr.fit(x,y)
model.score(x,y)

alphas=[1,10,20,100,200]
for a in alphas:
    r=Ridge(alpha=a)
    modelr=r.fit(x,y)
    score=modelr.score(x,y)
    print("alpha:",a,"score:",score)
    print("katsayılar:",modelr.coef_)